{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.datasets import load_iris, fetch_california_housing\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score, mean_squared_error\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\n\n# Suppress warnings\nwarnings.filterwarnings('ignore')\n\ndef compare_models(X, y, task='classification', param_grids=None):\n    # Check if task is valid\n    if task not in ['classification', 'regression']:\n        raise ValueError(\"Invalid task. Choose 'classification' or 'regression'.\")\n\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    # Standardize the data\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_val = scaler.transform(X_val)\n\n    # Define models\n    if task == 'classification':\n        models = {\n            'RandomForest': RandomForestClassifier(),\n            'GradientBoosting': GradientBoostingClassifier(),\n            'SVM': SVC(),\n            'LogisticRegression': LogisticRegression(max_iter=1000),\n            'DecisionTree': DecisionTreeClassifier(),\n            'KNN': KNeighborsClassifier(),\n            'Ridge': Ridge(),\n            'Lasso': Lasso()\n        }\n    else:\n        models = {\n            'RandomForest': RandomForestRegressor(),\n            'GradientBoosting': GradientBoostingRegressor(),\n            'SVM': SVR(),\n            'LinearRegression': LinearRegression(),\n            'DecisionTree': DecisionTreeRegressor(),\n            'KNN': KNeighborsRegressor(),\n            'Ridge': Ridge(),\n            'Lasso': Lasso()\n        }\n\n    baseline_results = []\n\n    # Evaluate baseline models\n    print(\"Evaluating baseline models...\")\n    for model_name in tqdm(models, desc=\"Baseline Models\"):\n        model = models[model_name]\n        if task == 'classification':\n            scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n            baseline_results.append({\n                'Model': model_name,\n                'Mean Score': scores.mean()\n            })\n        else:\n            scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n            baseline_results.append({\n                'Model': model_name,\n                'Mean Score': -scores.mean()\n            })\n\n    # Create a DataFrame with the baseline results\n    baseline_results_df = pd.DataFrame(baseline_results)\n    print(\"\\nBaseline Results:\\n\", baseline_results_df)\n\n    # Select the best model based on the baseline results\n    if task == 'classification':\n        best_model_name = baseline_results_df.loc[baseline_results_df['Mean Score'].idxmax()]['Model']\n    else:\n        best_model_name = baseline_results_df.loc[baseline_results_df['Mean Score'].idxmin()]['Model']\n    \n    best_model = models[best_model_name]\n\n    print(f\"\\nBest Model: {best_model_name}\")\n\n    # Hyperparameter tuning for the best model\n    if param_grids and best_model_name in param_grids:\n        print(f\"\\nPerforming hyperparameter tuning for {best_model_name}...\")\n        grid_search = GridSearchCV(best_model, param_grids[best_model_name], cv=5, scoring='accuracy' if task == 'classification' else 'neg_mean_squared_error', n_jobs=-1)\n        grid_search.fit(X_train, y_train)\n        best_model = grid_search.best_estimator_\n\n        print(f\"Best Parameters for {best_model_name}: {grid_search.best_params_}\")\n\n    # Evaluate the best model\n    y_pred = best_model.predict(X_val)\n    if task == 'classification':\n        final_score = accuracy_score(y_val, y_pred)\n        print(f\"\\nFinal Accuracy of the best model ({best_model_name}): {final_score}\")\n    else:\n        final_score = mean_squared_error(y_val, y_pred)\n        print(f\"\\nFinal Mean Squared Error of the best model ({best_model_name}): {final_score}\")\n\n    return best_model, baseline_results_df\n\n# Example parameter grids\nparam_grids = {\n    'RandomForest': {\n        'n_estimators': [50, 100, 150],\n        'max_depth': [None, 10, 20],\n        'min_samples_split': [2, 5, 10],\n        'min_samples_leaf': [1, 2, 4]\n    },\n    'GradientBoosting': {\n        'n_estimators': [50, 100, 150],\n        'learning_rate': [0.01, 0.1, 0.2],\n        'max_depth': [3, 5, 7],\n        'min_samples_split': [2, 5, 10],\n        'min_samples_leaf': [1, 2, 4]\n    },\n    'SVM': {\n        'C': [0.1, 1, 10, 100],\n        'gamma': [1, 0.1, 0.01, 0.001],\n        'kernel': ['linear', 'rbf']\n    },\n    'LogisticRegression': {\n        'C': [0.1, 1, 10, 100],\n        'penalty': ['l1', 'l2'],\n        'solver': ['liblinear', 'saga']\n    },\n    'LinearRegression': {},\n    'DecisionTree': {\n        'max_depth': [None, 10, 20],\n        'min_samples_split': [2, 5, 10],\n        'min_samples_leaf': [1, 2, 4]\n    },\n    'KNN': {\n        'n_neighbors': [3, 5, 7, 9],\n        'weights': ['uniform', 'distance']\n    }\n}\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T09:33:54.298889Z","iopub.execute_input":"2024-08-06T09:33:54.299277Z","iopub.status.idle":"2024-08-06T09:33:57.330037Z","shell.execute_reply.started":"2024-08-06T09:33:54.299247Z","shell.execute_reply":"2024-08-06T09:33:57.328849Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Load dataset for testing\nhousing = fetch_california_housing()\nX_regression, y_regression = housing.data, housing.target\n\n# Run the comparison for regression\nprint(\"\\n=== Regression Task ===\")\nbest_model_regression, baseline_results_df_regression = compare_models(X_regression, y_regression, task='regression', param_grids=param_grids)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T09:33:57.332297Z","iopub.execute_input":"2024-08-06T09:33:57.333188Z","iopub.status.idle":"2024-08-06T09:56:04.453011Z","shell.execute_reply.started":"2024-08-06T09:33:57.333145Z","shell.execute_reply":"2024-08-06T09:56:04.451401Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\n=== Regression Task ===\nEvaluating baseline models...\n","output_type":"stream"},{"name":"stderr","text":"Baseline Models: 100%|██████████| 8/8 [01:55<00:00, 14.49s/it]","output_type":"stream"},{"name":"stdout","text":"\nBaseline Results:\n               Model  Mean Score\n0      RandomForest    0.262886\n1  GradientBoosting    0.285267\n2               SVM    0.351828\n3  LinearRegression    0.519265\n4      DecisionTree    0.531783\n5               KNN    0.422986\n6             Ridge    0.519265\n7             Lasso    1.336930\n\nBest Model: RandomForest\n\nPerforming hyperparameter tuning for RandomForest...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Best Parameters for RandomForest: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 150}\n\nFinal Mean Squared Error of the best model (RandomForest): 0.25160074448312497\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Load dataset for testing\niris = load_iris()\nX_classification, y_classification = iris.data, iris.target\n\n# Run the comparison for classification\nprint(\"\\n=== Classification Task ===\")\nbest_model_classification, baseline_results_df_classification = compare_models(X_classification, y_classification, task='classification', param_grids=param_grids)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T09:56:04.454637Z","iopub.execute_input":"2024-08-06T09:56:04.455036Z","iopub.status.idle":"2024-08-06T09:56:07.019432Z","shell.execute_reply.started":"2024-08-06T09:56:04.454994Z","shell.execute_reply":"2024-08-06T09:56:07.018280Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\n=== Classification Task ===\nEvaluating baseline models...\n","output_type":"stream"},{"name":"stderr","text":"Baseline Models: 100%|██████████| 8/8 [00:02<00:00,  3.61it/s]\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nBaseline Results:\n                 Model  Mean Score\n0        RandomForest    0.950000\n1    GradientBoosting    0.950000\n2                 SVM    0.950000\n3  LogisticRegression    0.958333\n4        DecisionTree    0.950000\n5                 KNN    0.925000\n6               Ridge         NaN\n7               Lasso         NaN\n\nBest Model: LogisticRegression\n\nPerforming hyperparameter tuning for LogisticRegression...\nBest Parameters for LogisticRegression: {'C': 1, 'penalty': 'l2', 'solver': 'saga'}\n\nFinal Accuracy of the best model (LogisticRegression): 1.0\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n","output_type":"stream"}]}]}